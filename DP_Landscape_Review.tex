% F1000Research template from writeLaTeX

\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage{f1000_styles}
\usepackage{todonotes}

\begin{document}


\title{The Data Publication Landscape.}
\author[1]{John Ernest Kratz}
\author[1]{Carly Strasser}
\affil[1]{California Digital Library, University of California Office of the President, Oakland, CA 94612, USA}

\maketitle
\thispagestyle{fancy}

% Please list all authors that played a significant role in the research involved in the article. Please provide full affiliation information (including full institutional address, ZIP code and e-mail address) for all authors, and identify who is/are the corresponding author(s).

\begin{abstract}

The movement to incorporate datasets into the scholarly record as `first class' research products (validated, preserved, cited, and credited) has been building momentum for some time.
Data publications are beginning to appear all over the place, but there are still significant debates over formats, processes, and terminology.
This article will give an overview of the initiatives underway and the current conversation, highlighting places where consensus seems to have been reached and issues still in contention.

Data publication implementations follow variety of models that differ in, among other things, what kind of documentation is published, where the data resides relative to the documentation, and what validation is performed. Data can be published as a standalone product, as supplemental material to a traditional journal article, or with a descriptive ``data paper''.
Confusing the situation, terms are often used by different initiatives to refer to partially overlapping concepts.
The term `published' always means that the data is public and citable, but it may or may not mean peer reviewed.
In turn, data `peer review' can refer to substantially different processes.
There is substantial agreement on the elements of a basic dataset citation but a variety of solutions for citing subsets of datasets or datasets that change over time.
Finally, some are already looking past data publication to other metaphors, such as `data as software', for solutions to unsolved problems.

\end{abstract}
\clearpage


\section*{What does ``data publication'' mean?}\label{what-does-data-publication-mean}

The idea– and the ideal– that researchers should share data to advance knowledge and promote the common good is not at all new, but there is new enthusiasm for ``publishing'' data.
This shift in language from the casual ``share'' to the formal ``publish'' reflects a movement to fold datasets into the scholarly record with the same ``first class'' status as traditional research products like journal articles.
This goal is widely shared within the scholarly communication community,``Data publication'' is something of a buzzword, but different people and organizations mean different things by it.
A data publication might be an excel spreadsheet posted to a lab website, a set of images deposited in an institutional archive, a stream of readings from a weather station, or a peer-reviewed article describing a dataset hosted elsewhere, depending on who's speaking.

Among those who care (i.e. not most researchers), two properties of a data publication are almost universally agreed on: published data is \textbf{available} now and for the indefinite future, and it is formally \textbf{citable}. 
Less agreed on is the extent to which published data must be shown to be \textbf{trustworthy}.
In practice, availability is usually satisfied by depositing the dataset in a repository, citability by assigning a persistent identifier (such as a DOI), and validity by peer-review.

\section*{Why publish data?}\label{why-publish-data}

The movement toward data publication is driven by several goals.
One is to improve data preservation by moving datasets into stable repositories.
Another is to improve access to data by making it discoverable and citable.
The most significant is to increase the amount of data made public by tying into existing mechanisms for awarding credit to reward dataset publishers.
This is accompanied by a moral sense that the hard work of preparing and publishing data should be recognized.

\section*{What does a data publication look like?}\label{what-does-a-data-publication-look-like}

At present, a ``data publication'' may be any of a number of research objects published via a variety of processes.
Given the huge variety of types of data, it seems unlikely that any single structure will be ideal for every discipline and every dataset, but we can hope for a manageable number of blueprints. 
Models of data publication can be classified in a variety of ways, but for our purposes here, we will classify data publications into three categories based on the accompanying documentation; a dataset may \textbf{supplement} a traditional research paper, be the \textbf{subject} of a ``data paper'', or be \textbf{independent} of any paper.

\subsection*{Paper supplement data}\label{paper-supplement-data}

The most familiar model to researchers is data published along with a traditional journal article. 
The article's publisher typically hosts the dataset as supplementary material, but that practice is in decline.
For example, the Journal of Neuroscience stopped publishing supplemental material in 2010\cite{maunsell_announcement_2010}. 
Journal publishers have generally not been interested in the work of ensuring data preservation, and many prefer that someone else handle it.

The most prominent repository exclusively for making supplemental data public is Dryad\cite{dryad}.
Dryad originated in the ecology and evolutionary biology community, but has grown to accept data underlying any peer-reviewed or otherwise ``reputable'' publication. Dryad makes data available and citable, but any assessment of scientific validity must be managed by the publisher of the article. 

For some specific data types, deposition in appropriate databases has been the standard for a long time. 
For example, DNA sequences are deposited in GenBank\cite{genbank} and protein structures in the Protein Data Bank\cite{protein_data_bank}.

\todo[inline]{reproducibility crisis}
\todo[inline]{PLOSFail}

\subsection*{Paper subject data}\label{paper-subject-data}

A \textbf{data paper} describes a dataset by thoroughly detailing the rationale and collection methods, but lacks any results or conclusions. 
Data papers are flourishing in new journals dedicated to the format, such as Nature Scientific Data\cite{nature_scientific_data}; Geoscience Data Journal\cite{geoscience_data_journal}; and a trio of ``metajournals'' from Ubiquity Press\cite{ubiquity_press_metajournals}, and also in journals that publish other types of papers, such as F1000 Research\cite{f1000_research}; Internet Archaeology\cite{internet_archaeology}; and GigaScience\cite{gigascience}. 
The length and structure of data papers varies significantly between journals, and the format is most clearly defined by what is absent.
With few exceptions, data journals require datasets to be published by a trusted third-party repository. 
\todo{Consider referencing an exception or two}
\todo{Consider mentioning NSD's list of approved repositories}
Data papers are peer-reviewed, as will be discussed later.  At present, roughly half take the novelty or potential impact of a dataset into consideration, while the others only require that the data be scientifically valid.

\subsection*{Paper independent data}\label{paper-independent-data}

To be useful or reproducible, a dataset must have accompanying descriptive information (i.e.~metadata), but this needn't take the form of a journal article. 
Datasets can be published by a repository or with rich structured or freeform metadata colocated with the dataset instead of an associated journal article. Repositories are able to provide access and citability, but the degree of validation varies widely. 
Few are equipped to provide peer-review. Figshare\cite{figshare}, for instance, publishes datasets without any validation (although a Figshare dataset associated with a data paper will have been reviewed along with the paper). 
On the other hand, Open Context\cite{open_context} publishes very high quality archeology datasets with optional peer review.

% \section*{How does publication work?}\label{how-does-publication-work}

\section*{Availability}\label{availability}

To publish is to make public, and to make data available is the critical function of data publication. 
Published data must be available not only now, but in the future, which requires preservation as well as access. 
It should be noted that there is no requirement that published data be available to anyone at no cost.

As a practical matter, publishing a dataset means depositing it in a trustworthy repository. 
What constitutes ``trustworthy'' is up to you. 
A number of repository certification schemes exist. The most thorough is Trusted Repository Audit Checklist (TRAC)\cite{trac_2007} certification by the Center for Research Libraries, but that's so onerous that only four repositories have gone thorough the process. 
Probably a more typical way to decide trustworthiness is to judge by the organization running it. 
Repositories run by governments or large universities might be considered trustworthy (although the effects of the 2013 US government shutdown on PubMed might give one pause).
\todo{add mention of DUA, perhaps \$}


\section*{Citability}\label{citability}

If a researcher uses a published data set in a paper, she should cite the dataset formally in the reference list. 
Citation is perhaps the element of data publication that has come the farthest toward establishing consensus.
A variety of sets of principles and recommendations from different groups have been synthesized into a recently-released consensus set of Data Citation Principles. \cite{force11_data_2014} 
However, there is still disagreement about how to handle edge cases.

Data publications have to facilitate citation. 
This is generally facilitated by assigning a unique permanent identifier, most commonly a DOI, to the dataset. 
As long as the DOI is maintained, it can be used by anyone interested to locate the dataset. 
(It is worth pointing out that a DOI is neither sufficient nor necessary for citability-- if the DOI is not maintained, the citation breaks and, conversely a well maintained URL works just as well as a DOI). 
The identifier will generally be something, such as a DOI, that can be used to locate the referenced object, as such it can be thought of as replacing the volume and page number used to find an article in a print journal.

\subsection*{Simple Case}\label{simple-case}

In the simplest case, there is substantial agreement that a published dataset should be cited using five elements largely familiar from journal citations: creator(s), title, year, publisher and identifier. 
This format is consistent with the recommendation made by CODATA\cite{socha_out_2013} and with metadata required by DataCite \cite{datacite_datacite_2013} and Thomson Reuters Data Citation Index. However, this article-descended formulation is not adequate to address some of the complications unique to datasets.

\subsection*{Deep Citation}\label{deep-citation}

The first major complication that datasets face is the need for deep citation. 
When supporting an assertion in writing, it is considered sufficiently precise to cite the entirety of the referenced journal article and leave it to the suspicious reader to identify the basis of your assertion. 
If only part of a dataset is used in a quantitative analysis, you may need to specify exactly the subset in question. 
Because datasets are so variable in structure, there will probably not be a general solution. 
The most common approach is to cite the entire dataset and describe the subset in the text of the paper. 
In some cases, it may be practical to include a date or record number range or a list of variables in the formal citation.

\subsection*{Dynamic Data}\label{dynamic-data}

A second complication is that datasets are prone to existing in multiple versions or changing over time. 
In the past, the printed article was a single version of record.
Web based publishing and preprint servers such as arXv.org have already complicated the matter. 
Data publishers are likely to allow or even encourage updating and correction of datasets. 
For the results of data analysis to be reproducible, the reader must be able to obtain precisely the version of the data that the researcher used. 
In the case of dynamic data, that means that previous versions have to be preserved and citable.

As a practical matter, there are two kinds of dynamic data that warrant consideration: expanding datasets, to which new data may be added but old data will never be changed or deleted, and revisable datasets in which data may be added, deleted, or changed over time. 
Common solutions to add-on data are to include an access date, or a date or record number range in the citation. 
Revisable datasets are more difficult, but the most common approach is to periodically publish multiple changes as a new version with a version number that can be included in citations.

Controversy persists about dynamic data and identifiers and different publishers have different policies. 
DataCite recommends but does not require that the DOIs that they issue point to immutable objects. 
Dataverse, for example, (check up) does not permit changes, but instead recommends that growing datasets be issued a new DOI periodically that refers to the ``time-slice'' of records added since the last DOI was issued; revisable datasets are to be periodically frozen as a ``snapshot'' and issued a new DOI.

\subsection*{Just-in-time Identifiers}\label{just-in-time-identifiers}

One potential solution to both deep citation and dynamic data is to turn the identifier-issuing process on its head. 
Instead of a dataset publisher minting the identifier, the researcher who wants to cite a dataset could mint an identifier that refers to precisely the part of the dataset that they wish to cite. 
The Research Data Alliance (RDA) Data Citation Working Group has put fort a sophisticated proposal suitable for database in which an identifier would wrap together a number of components including specifying a version of the database and a query over the database that produces the cited dataset. 
This seems promising, but there are still many technical and policy issues that have to be resolved before this can be widely adopted.

\section*{Trustworthiness}\label{trustworthiness}

% \subsection*{Peer-review}\label{peer-review}

%For journal articles, peer-review is the gatekeeper to the scholarly record, meant to ensure some level of trustworthiness.
%In many fields formally peer-reviewed literature enjoys a much higher status than even reputable ``grey literature''.
%The effort to apply the prestige of ``publication'' to datasets cascades into an effort to apply the prestige of ``peer-review'' to data.
%Like data publication, data peer-review is being defined now.

%What does it mean to peer-review data?

\subsection*{Technical vs. scientific review}
Callaghan draws a useful distinction here: between technical and scientific review. \cite{callaghan_making_2012}
Technical review assures us that the dataset has complete metadata, no missing values that aren't allowed to be missing, etc. and generally doesn't require domain expertise. 
Scientific review evaluates the methods of data collection, the overall plausibility of the data, and the likely reuse value. 
Both kinds of review can be done together, or, in the case of a data paper, it's common for the repository to do the technical review and the data journal to do the scientific review.

\subsection*{Supplement data review}
Traditionally, peer reviewers haven't had access to the raw data underlying a paper, so that hasn't been part of the review process.
As more journals require the data underlying publications to be made public, this has the potential to change, but it's unclear whether reviewer practices are changing.
Encouragingly, a recent \emph{F1000Research} survey of peer reviewers found that 90\% reported examining the underlying data.
\todo[inline]{contact F1000 to figure out what to cite}

\subsection*{Paper subject data review}
Publishers of data papers wrap peer review of the paper and of the dataset together. 
An exception is GigaScience\cite{gigascience}, which assigns a separate data reviewer for technical review of the dataset. 
Reviewer guidelines are roughly similar across journals, although roughly half of the journals we looked at consider novelty or potential impact, while the others only require that the dataset be scientifically sound.
While review guidelines are similar, review processes are not. 
Data paper peer review processes range from traditional to experimental (open post-publication review in F1000 Research).

As an example, let's compare Nature Scientific Data and Biodiveristy journal.
The two journals divide reviewer guidelines into three similar sections– quality of the data, quality of the description, and consistency between the description and the data– and provide similar guidance.
However, their peer review processes are quite different.
NSD implements a traditional peer review process: the editor appoints 1 or more reviewers, who are encouraged to remain anonymous.
Biodiversity Journal has a more flexible and open process.
There, anonymity is up to each reviewer, and there are multiple classes of reviewer.
the editor appoints two or three ``nominated'' reviewers who are required to supply feedback and several ``panel'' reviewers who read the paper and only supply feedback if they feel like they have something to say.
Additionally, the authors may opt to open the paper to public comment during the review process.


\subsection{Independent data review}
More interesting yet are review processes for standalone datasets. 
NASA Planetary Data System (PDS)\cite{nasa_pds} conducts peer review in an in-person meeting with representatives of the repository, the dataset creators, and the reviewers. 
Open Context goes beyond the simple accept/reject binary of traditional peer review. 
Instead, each dataset has a rating from 1-5 that indicates how thoroughly it has been reviewed. 
Essentially, a 3 indicates that the dataset has passed technical review, a 4 means that it has passed editorial review, and a 5 means that it has passed external peer review.

% \subsection*{Post-publication review}\label{post-publication-review}

\section*{Beyond data publication}\label{beyond-data-publication}

Parsons and Fox\cite{parsons_is_2013} argue that the metaphor of ``publication'' is limiting, and only suited to some datasets. 
They identify a number of analogies other than scholarly communication that might apply to data, including industrial production, cartography, and the World Wide Web.
They also make reference to an analogy that seems to be gaining momentum: data as software.\cite{schopf_treating_2012} 


Under this metaphor, publishing a dataset is analogous to a software release, and subsequent changes are analogous to new versions.
The open source software community has already confronted many of the problems associated with data (managing versions, sharing, collaboration) and developed tools and approaches to address them.

\cite{ram_git_2013}
\cite{chen_close_2014}
Open context came to use Mantis bug tracking software and Git with their data out of practical concerns.

Still issues to address: Current VCSs are designed for code (relatively small text files), not large and variegated datasets. 
Attribution for derived datasets is not clear, but that's likely to be a cultural issue.


\nocite{*}
{\small\bibliographystyle{unsrt}
\bibliography{sample}}

\listoftodos

% See this guide for more information on BibTeX:
% http://libguides.mit.edu/content.php?pid=55482&sid=406343

% For more author guidance please see:
% http://f1000research.com/author-guidelines


% When all authors are happy with the paper, use the 
% ‘Submit to F1000Research' button from the Share menu above
% to submit directly to the open life science journal F1000Research.

% Please note that this template results in a draft pre-submission PDF document.
% Articles will be professionally typeset when accepted for publication.

% We hope you find the F1000Research writeLaTeX template useful,
% please let us know if you have any feedback using the help menu above.


\end{document}