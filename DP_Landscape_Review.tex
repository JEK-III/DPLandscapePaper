% F1000Research template from writeLaTeX

%\documentclass[10pt,a4paper,twocolumn]{article}
\documentclass[10pt,twocolumn]{article}

\usepackage[superscript]{cite}
\usepackage{f1000_styles}
\usepackage{hyperref}

\hypersetup{colorlinks=true}


\begin{document}


\title{The Data Publication Landscape.}
\author[1]{John Ernest Kratz}
\author[1]{Carly Strasser}
\affil[1]{California Digital Library, University of California Office of the President, Oakland, CA 94612, USA}

\maketitle
\thispagestyle{fancy}

% Please list all authors that played a significant role in the research involved in the article. Please provide full affiliation information (including full institutional address, ZIP code and e-mail address) for all authors, and identify who is/are the corresponding author(s).

\begin{abstract}


The movement to incorporate datasets into the scholarly record as first class research products (validated, preserved, cited, and credited) has been slowly building momentum for some time, but the pace of developments picked up substantially in the last year.
Data publications are proliferating, but there are still significant debates over formats, processes, and terminology.
This article will present an overview of the initiatives underway and the current conversation, highlighting places where consensus seems to have been reached and issues still in contention.

Data publications follow a variety of models that differ in, among other things, what kind of documentation is published, where the data lives relative to the documentation, and how the data is validated.
Data can be published as supplemental material to a journal article, with a descriptive ``data paper'', or independently.
Further complicating the situation, the same terms are used by different initiatives to refer to related by distinct concepts.
The term `published' means that the data is public and citable, but it may or may not mean peer reviewed.
In turn, data `peer review' can refer to substantially different processes, although data paper referee guidelines are fairly uniform.
There is substantial agreement on the elements of a dataset citation (which closely resembles that of a journal article) but a variety of solutions for citing subsets of datasets or datasets that change over time.
Finally, some are already looking past data publication to other metaphors, such as `data as software', for solutions to unsolved problems.

\end{abstract}
\clearpage



\section*{Introduction: what does data publication mean?}\label{introduction}

The idea that researchers should share data to advance knowledge and promote the common good is an old one, but in recent years the conversation has shifted from sharing data to ``publishing'' data.\cite{costello_motivating_2009,smith_data_2009,lawrence_data_2011}
The shift in language is driven by the conviction that datasets should be brought into the scholarly record and afforded the same first class status as traditional research products like journal articles.\cite{sarah_callaghan_making_2012}
While this goal is widely shared within the scholarly communication community, different people and organizations imply different things by the term \emph{data publication}.

The community largely agrees on two essential properties of a data publication.\cite{smith_data_2009,sarah_callaghan_making_2012}
First, published data is publicly \textbf{available} now and for the indefinite future; access might require payment of fees or acceptance of a legal agreement, but not the approval of the author.
Second, like a book or journal article, a data publication can be formally \textbf{cited}.
Open questions surround a third property: how and to what extent a published dataset must be \textbf{validated}.
Callaghan (2012)\cite{sarah_callaghan_making_2012} draws a distinction between data that has been shared, published (lower-case “p”), or Published (upper-case “P”): \textbf{shared} data is available, \textbf{published} data is available and citable, and \textbf{Published} data is available, citable, and validated.
In practice, availability is usually satisfied by depositing the dataset in a repository, citability by assigning a persistent identifier (e.g. a Digital Object Identifier), and validity by peer-review.

\subsection*{Why publish data?}\label{why-publish-data}

The underlying goals of data publication are to enable research \textbf{reproduction} and to facilitate data \textbf{reuse}.
Hidden primary data exacerbates science's very public ``reproducibility crisis''.\cite{mobley_survey_2013,pashler_is_2012,zimmer_rise_2012,hiltzik_science_2013,begley_drug_2012}
Widespread publication of the data underlying research papers could help expose both fraud and honest errors.\cite{drew_lost_2013}
The leaders of the US National Institutes of Health (NIH) recently cited ``provid[ing] greater transparency of the data that are the basis of published manuscripts'' as one way to improve scientific reproducibility.\cite{collins_nih_2014}

Although there can be no substitute for investing time and funding in the collection of new data, appropriate reuse can lower costs and accelerate research. 
Storing and publishing data with adequate documentation is costly in terms of both time and funding, but preparation and storage costs are usually far less than than the cost of collecting the data again.
Open Context published data from a site in eastern Turkey at the substantial cost of \$10,000-15,0000, but the publication cost is minor compared to \$800,000 spent on the data collection.\cite{kansa_we_2014}
Piwowar (2011) contrasted \$100,000 in National Science Foundation (NSF) grants, which generate an average of 3-4 papers with an estimate that the same investment in curating, archiving, and publishing data could contribute to over 1,000 publications.\cite{piwowar_data_2011}
Most critcially, time-variant data (e.g. climate records and observations of unique astronomical events) can never be recreated for any price.\cite{gray_online_2002}

Journals frequently require authors to supply underlying data on request already.
As of 2011, 88\% of high-impact publications required a statement regarding the availability of underlying data; half of those made willingness to provide data a condition of publication.\cite{alsheikh-ali_public_2011}
However, the authors of 59\% of papers examined failed to adhere to the availability instructions.
Vines et al. (2014)\cite{vines_availability_2014} could only obtain underlying data from 101 of 516 papers published from 1991 to 2011.
Availability dropped off sharply with time; data could only be obtained from 2 of the 62 oldest papers.
Now, journals are beginning to require that underlying data be published simultaneously with the article.
In 2010, a coalition of Ecology and Evolutionary Biology journals began to require that the data underlying articles be archived with a maximum embargo of 1 year.\cite{whitlock_data_2010, fairbairn_advent_2010}
\href{http://f1000research.com}{\emph{F1000Research}} has had a similar policy (without an embargo period) since its inception, and the {\href{http://www.plos.org/}{Public Library of Science (PLOS)}} journals followed suit this year.\cite{bloom_data_2014}

\section*{Types of data publication}\label{types-of-data-publication}

The still-solidifying phrase ``data publication'' covers diverse kinds of research objects published via a variety of processes.
Depending on the speaker, a data publication might be an excel spreadsheet on a website, a set of images in an institutional archive, a stream of readings from a weather station transmitted over the internet, or a peer-reviewed article describing a dataset hosted elsewhere.
Different disciplines, subdisciplines, and indvidual researchers consider different assortments of digital material to their data, and it is unlikely that any single structure will be ideal for every discipline and dataset, but we can hope to agree on a manageable number of blueprints.
For instance, Lawrence (2011) draws five data publication models ``discriminated in the main by how the roles involved in publication are distributed between the various actors'' (e.g. the author, archive or journal).\cite{lawrence_data_2011}
Here, we will more simply group data publications into three categories based on the accompanying documentation; a dataset may \textbf{supplement} a traditional research paper, be the \textbf{subject} of a ``data paper'', or be \textbf{independent} of any paper.

\subsection*{Data that supplements a paper}\label{paper-supplement-data}

For researchers, the most familiar kind of data publication is a traditional journal article accompanied by the underlying data.
The data can be hosted by the journal publisher as supplementary material or deposited in a third-party repository.
Repositories are generally considered to be better suited to ensure long-term preservation and access to the data, and the trend is away from supplemental material. 
For instance, The Journal of Neuroscience stopped publishing supplemental material in 2010.\cite{maunsell_announcement_2010}
The most prominent repository for data underlying any peer-reviewed or otherwise ``reputable'' publications is \href{http://www.datadryad.org/}{Dryad}. 
Dryad makes data available and citable, but the publisher of the article manages any assessment of scientific validity.
Other third-party repositories include \href{http://figshare.com/}{figshare}, \href{http://zenodo.org/}{Zenodo}, institutional repositories (e.g. the \href{https://purr.purdue.edu/}{Purdue Research Repository}), and discipline-specific repositories (e.g. DNA sequences are deposited in \href{http://www.ncbi.nlm.nih.gov/genbank/}{GenBank}\cite{benson_genbank_2013} and protein structures in the \href{http://www.rcsb.org/}{Protein Data Bank}\cite{berman_protein_2000}).


\subsection*{Data as the subject of a paper}\label{paper-subject-data}

A \textbf{data paper} describes a dataset by thoroughly detailing the rationale and collection methods, but lacks any analysis or conclusions.\cite{newman_data_2009}
Data papers are flourishing as a new article type in journals such as \emph{F1000Research}, \href{http://www.internetarchaeology.org/}{\emph{Internet Archaeology}}, and \href{http://www.gigasciencejournal.com/}{\emph{GigaScience}}\cite{gigascience}, and in new journals dedicated to the format like \href{http://onlinelibrary.wiley.com/journal/10.1002/%28ISSN%292049-6060}{\emph{Geoscience Data Journal}}\cite{geoscience_data_journal}, a trio of ``metajournals'' from Ubiquity Press, and Nature Publishing Group's forthcoming \href{http://www.nature.com/scientificdata/}{\emph{Scientific Data}}.

The length and structure of data papers varies significantly between journals, but the tendency is toward relatively short and structured papers.
All of them feature an abstract, collection methods, and description of the dataset; a few (e.g. \emph{Internet Archaeology}, \href{http://openhealthdata.metajnl.com/about/submissions#authorGuidelines}{\emph{Open Health Data}}) encourage authors to suggest potential uses for the data.
In addition to this general framework, some journals incorporate sections specific to the field; for instance, \emph{Internet Archaeology} and the \href{http://openarchaeologydata.metajnl.com/}{\emph{Journal of Open Archaeology Data}} both include a separate section to describe the  temporal or geographic scope of the dataset.
Data papers are best delineated from traditional articles not by the presence of any particular information, but by the absence of analysis and conclusions.
A sharp distinction is needed because many publishers (e.g. ones on a \href{https://f1000research.com/data-policies}{list} complied by \emph{F1000Research}) do not consider a data paper as a prior publication, should the authors seek to publish a subsequent analysis paper.

With few exceptions, data journals require datasets to be published by a trusted third-party repository.
\emph{GigaScience} has a tightly associated repository, \emph{GigaDB}, to host datasets, and \emph{The International Journal of Robotics Research}\cite{international_journal_of_robotics_research}, an early comer to data papers\cite{newman_data_2009}, allows authors to host datasets on their own websites, but more typcially, \emph{Scientific Data} and \emph{Geoscience Data Journal} list approved disciplinary and general-purpose third-party repositories in their instructions for authors.

\subsection*{Data independent of any paper}\label{paper-independent-data}

To be useful or reproducible, a dataset must have accompanying descriptive information (i.e. metadata)\cite{gray_online_2002}, but this needn't take the form of a journal article.
Datasets can be published by a repository, or with rich structured or freeform description colocated with the dataset instead of an associated journal article.
Repositories are able to provide access and citability, but the degree of validation varies widely.
Few are equipped to provide peer-review.
Figshare, for instance, publishes datasets without any validation (although a Figshare dataset associated with a data paper will have been reviewed along with the paper).
Like Figshare, \href{http://www.openaire.eu/en/home}{Open Access Infrastructure for Research in Europe (OpenAIRE)} and the \href{http://home.web.cern.ch/}{European Organization for Nuclear Research (CERN)}'s \href{https://zenodo.org/}{ZENODO} publishes any research output, including datasets.

The full range of validation is found in the field of Archaeology.
\href{http://www.tdar.org/}{The Digital Archaeological Record (tDAR)} focuses on acess and preservation and publishes data with no peer-review.
However, tDAR can function as the third party repository for data papers from \emph{Internet Archaeology} or the \emph{Journal of Open Archaeology Data}.
\href{http://opencontext.org/}{Open Context} publishes very high quality archeology datasets with optional peer review.



\section*{Availability}\label{availability}

To publish is to make public; at its most fundamental, to publish data is to make data public.
For published data to be available in the future, both preservation and access mechanisms are required.\cite{beagrie_digital_2008, gray_online_2002}
As in print publication, there is no requirement that published data be free or legally unencumbered, and it is not uncommon to require acceptance of a data use agreement.
When access is limited, it should be contigent only on clear and objective criteria for access; writing the creator for permission should not be part of the process.
For instance, \href{http://www.icpsr.umich.edu/icpsrweb/content/deposit/confidentiality.html}{The Inter-university Consortium for Political and Social Research (ICPSR)} social science data archive evaluates a researcher's securtiy measures before granting access to restricted data, but does not judge the validity of the proposed research.
The need to protect the privacy of human subjects by far the most common source of access restrictions.

As a practical matter, publishing a dataset usually means depositing it in a trustworthy repository.
What constitutes a trustworthy repository is largely subjective, but some measures can be agreed on.
Multiple certification schemes exist.
The most extensive is the Trusted Repository Audit Checklist (TRAC)\cite{dale_trustworthy_2007} from the Center for Research Libraries, but TRAC certification is so onerous that only four repositories have gone thorough the process.
The \href{http://datasealofapproval.org/}{Data Seal of Approval}, created by the Dutch \href{http://www.dans.knaw.nl/en}{Data Archiving and Networked Services (DANS)} but administered by an international board, has been awarded to 24 repositories following a considerably more streamlined process.
The guidelines were first published in 2010.
A more typical way to decide trustworthiness is to judge by the organization running it.
Repositories run by governments or large universities are likely to be considered trustworthy (although the effects of the 2013 US government shutdown on PubMed might give one pause).
In contrast, independent repositories such as figshare may be riskier in the long term. %a wise thing to say?  actual failure might be better

\section*{Citability}\label{citability}

Citation is perhaps the element of data publication that has come the farthest toward establishing consensus.
This year, a coalition of groups involved in scholarly communication– including Future Of Research Communication and E-Scholarship (FORCE11)\cite{bourne_improving_2012}, the Committee on Data for Science and Technology (CODATA)\cite{codata-icsti_task_group_on_data_citation_standards_and_practices_out_2013}, the Digital Curation Center (DCC), and others– released a \href{http://www.force11.org/datacitation}{Joint Declaration of Data Citation Principles}.
The first of the 8 principles states, in part, that ``[d]ata citations should be accorded the same importance in the scholarly record as citations of other research objects, such as publications''
Most of the time, this means that if researcher uses a published data set in a paper, they should cite the dataset formally in the reference list.
	
Data publications generally enable citation by assigning a unique permanent identifier to the dataset.
Most commonly, this is the same identifier used for journal articles, a Digital Object Identifier (DOI).
For as long as the DOI is maintained, anyone interested can use it to locate the dataset.
Note, however, that a DOI is neither sufficient nor necessary for citability-- if the DOI is not maintained, the citation breaks and, conversely a well maintained URL works as well as a DOI.

\subsection*{Simple Case}\label{simple-case}

In the simplest case, there is substantial agreement that a published dataset should be cited using five elements largely familiar from journal citations: creator(s), title, year, publisher and identifier.
This format is consistent with the example accompanying the Joint Declaration of Data Ciation Principles, the recommendation made by CODATA\cite{codata-icsti_task_group_on_data_citation_standards_and_practices_out_2013} and with the information requried to obtain a DataCite DOI\cite{datacite_datacite_2013} or be listed in the \href{http://thomsonreuters.com/data-citation-index/}{Thomson-Reuters Data Citation Index}.
However, this article-descended formulation is not adequate to address some of the complications unique to datasets.

\subsection*{Deep Citation}\label{deep-citation}

The first major complication that datasets face is the need for \textbf{deep citation}.
When supporting an assertion in writing, it is considered sufficiently precise to cite the entirety of the referenced journal article and leave it to the suspicious reader to identify the basis of your assertion.
If only part of a dataset is used in a quantitative analysis, one may need to specify exactly the subset in question.
Because datasets are so variable in structure, a general solution may not exist.
The most common suggestion is to cite the entire dataset and describe the subset in the text of the paper.\cite{altman_a_2007,}
In some cases, it may be practical to include a date or record number range or a list of variables in the formal citation, e.g. as suggested by the  \href{http://wiki.esipfed.org/index.php/Interagency_Data_Stewardship/Citations/provider_guidelines#Subset_Used}{Federation of Earth Science Information Parterns (ESIP)} or the \href{http://nsidc.org/about/use_copyright.html}{National Snow and Ice Data Center (NSIDC)}.

\subsection*{Dynamic Data}\label{dynamic-data}

A second complication is that datasets are prone to existing in multiple versions or changing over time.
Consider, for example, the dataset collected by a the sensors in a weather station, which expands every time a reading is recorded.
Or consider the set of all known and predicted mRNAs from an organism's genome, which is periodically revised as experimental information comes in and predictive models are improved.
In the past, the printed article was a single version of record.
Web based publishing and preprint servers such as \href{http://arxiv.org/}{arXiv.org} have already complicated the matter, but data publishers are especially likely to allow or encourage updates and corrections.
The ability to update datasets is usefule, but for the results of data analysis to be reproducible, the reader must be able to obtain precisely the version of the data that the researcher used.
In the case of \textbf{dynamic data}, that means that previous versions have to be preserved and citable.

As a practical matter, there are two kinds of dynamic data that warrant consideration: \textbf{expanding} datasets, like the weather readings, to which new data may be added but old data will never be changed or deleted, and \textbf{revisable} datasets, like the mRNAs, in which data may be added, deleted, or changed over time.
Common solutions to expanding data are to include an access date, or a date or record number range in the citation.
Revisable datasets are more difficult, but the most common approach is to periodically publish multiple changes as a new version with a version number that can be included in citations.

Controversy persists about dynamic data and identifiers and different publishers have different policies.
DataCite recommends but does not require that the DOIs that they issue point to immutable objects.
The DCC, for example, recommends that growing datasets be issued a new DOI periodically that refers to the ``time-slice'' of records added since the last DOI was issued; revisable datasets are to be periodically frozen as a ``snapshot'' and issued a new DOI.

\subsection*{Just-in-time Identifiers}\label{just-in-time-identifiers}

One potential solution to both deep citation and dynamic data is to turn the identifier-issuing process on its head.
Instead of a dataset publisher minting the identifier, the researcher who wants to cite a dataset could mint an identifier that refers to precisely the part of the dataset that they wish to cite.
The Research Data Alliance (RDA) \href{http://rd-alliance.org/working-groups/data-citation-wg.html}{Data Citation Working Group} has put forth a sophisticated proposal suitable for databases in which an identifier would wrap together a number of components including specifying a version of the database and a query over the database that produces the cited dataset.
This seems promising, but there are still many technical and policy issues that have to be resolved before this can be widely adopted.

\section*{Validation}\label{validation}

How to handle data validation is the least clear aspect of data publication.
For journal articles, peer-review is the gatekeeper to the scholarly record, meant to ensure some level of validity.
In many fields formally peer-reviewed literature enjoys a much higher status than even the most reputable grey literature.
The effort to apply the prestige of ``publication'' to datasets cascades into an effort to apply the prestige of ``peer-review'' to data.
However, at the same time that we're trying to develop and anaolog of journal peer-review for data, article peer-review itself is in flux.
Internet-enabled experiments are going on, like the open peer review at \emph{F1000Research}, or \href{http://www.ncbi.nlm.nih.gov/pubmedcommons/}{PubMed Commons}, a platform for post-publication commenting on biomedical literature.
At a minimum, data review creates new issues, which we're only beginning to get a handle on.

Callaghan (2012)\cite{sarah_callaghan_making_2012} draws a useful distinction between \textbf{technical} and \textbf{scientific} review.
Technical review verifies that the dataset is complete, the description is complete, and that the two match up.
Technical review generally doesn't require domain expertise, and many repositories provide at least some level of review.
Scientific review evaluates the methods of data collection, the overall plausibility of the data, and the likely reuse value.
Scientifc review requires domain expertise and is more difficult to organize, so few repositories provide it.
In the case of a data paper, it's common for the repository to do the technical review and the data journal to do the scientific review.

\subsection*{Data paper peer-review}\label{data-paper-peer-review}

Peer-review of a paper certifies that the methods are appropriate to address the questions and the data support the conclusions.
In the absence of particular questions and conclusions, it is unclear what peer-review should certify.
A dataset may be good enough for one purpose, but not for another.\cite{parsons_data_2010}
While a reviewer can be expected to read an entire article, they certainly can't inspect every point in a large dataset.
Furthermore, researchers are already overburdened by peer-review of articles, so increasing their workload may be unreasonable.
These problems are being tackled by a number of publishers.

Publishers of data papers wrap peer review of the paper and the dataset together.
An exception is \emph{GigaScience}, which assigns a separate data reviewer for technical review of the dataset.
Reviewer guidelines are roughly similar across journals, with the exception that roughly half of the journals we looked at consider novelty or potential impact, while the others only require that the dataset be scientifically sound.
While review guidelines are similar, review processes are not.

As an example, consider \emph{Scientific Data} and \emph{Biodiveristy Journal}.
The two journals each divide reviewer guidelines along similar lines into three sections, which \emph{Biodiviersity Journal} calls ``quality of the data'', ``quality of the description'', and ``consistency between manuscript and data.''
However, the peer review processes diverge.
\emph{Scientific Data} follows a traditional peer review process: the editor appoints one or more reviewers, who are encouraged to remain anonymous.
\emph{Biodiversity Journal} has a more flexible and open process featuring multiple classes of reviewer and entirely optional annonymity.
The editor appoints two or three ``nominated'' reviewers who are required to supply feedback and several ``panel'' reviewers who read the paper and only supply feedback if they feel like they have something to say.
Additionally, the authors may choose to open the paper to public comment during the review process.
Review processes for standalone datasets are less clearly defined.

\subsection*{Independent data vaidation}

While data journals all model their data validation more or less faithfully on traditional peer-review, independent data validation practices and proposals are considerably more varied
On the concervative end of the spectrum, Lawrence (2011) proposes out a set of criteria for data peer-review.\cite{lawrence_citation_2011}
The \href{https://pds.jpl.nasa.gov/}{Planetary Data System (PDS)}\cite peer-reviews datasets, but through the unusual process of holding an in-person meeting with representatives of the repository, the dataset creators, and the reviewers.
Open Context also incorporates peer-review, but optionally and in a way that goes beyond the a simple accept/reject binary.\cite{kansa_we_2013}
Instead, each dataset has a rating from 1-5 that indicates how thoroughly it has been reviewed.
Essentially, a 3 indicates that the dataset has passed technical review, a 4 means that it has passed editorial peer-review, and a 5 means that it has passed external peer review.

Post-publication feedback from succesful or unsuccessful reusers can offer a substitute for or supplement to pre-publication validation.
Parsons et al. (2011) suggest that ``data use in its own right provides a form of review,'' and goes on to point out that doucmenting the context of reuse demonstrates that the data is not only good, but good for some particular application.\cite{parsons_data_2011}
DANS solicts structured, multifaceted feedback from users of their datasets: users are asked to assign a rating on a five star scale for each of six criteria (e.g., data quality, quality of the documentation, structure of the dataset).\cite{grootveld_data_2011,grootveld_peer-reviewed_2012}



\section*{Beyond data publication}\label{beyond-data-publication}

In a 2013 paper\cite{parsons_is_2013}, Parsons and Fox argue that thinking about data through the the metaphor of print ``publication'' is potentially very limiting.
Diverse kinds of material are regarded as data by one research community or another, and while at least some aspects of publication apply well to at least some kinds of data, there are many other possible approaches.
One alternative metaphor that seems to be gaining traction is data as software.\cite{schopf_treating_2012}

In some cases, it may be better to think of releasing a dataset as one would a piece of software, and to regard subsequent changes are analogous to updated versions.
The open source software community has already developmed many tools for working collaboratively, managing mutliple versions, and tracking attribution.
Ram (2013)\cite{ram_git_2013} catalogs a multitude of scientific uses for the software version control system \href{http://git-scm.com/}{Git}, including for managing data.
Open context came as a practical matter to use Git and \href{http://www.mantisbt.org/}{Mantis Bug Tracker} to track and correct dataset errors.
Furthermore, projects such as \href{http://ipython.org/notebook}{IPython Notebook} integrate data, processing, and analysis into a single package.
However, scientific software is struggling for recognition\cite{pradal_publishing_2013} just as data is, so revising the academic reward system continues to be a challenge.

Ultimately, while ``data as software'' is promising, data is neither literature nor software, and
The prestige and familiarity of ``publication'' and ``peer-review'' are extremely useful, but it may be necessary to stretch the definitions of each to encompass processes that work for data.


%\nocite{*}
{\small\bibliographystyle{unsrt}
\bibliography{LandscapePaper}



% See this guide for more information on BibTeX:
% http://libguides.mit.edu/content.php?pid=55482&sid=406343

% For more author guidance please see:
% http://f1000research.com/author-guidelines


% When all authors are happy with the paper, use the
% ‘Submit to F1000Research' button from the Share menu above
% to submit directly to the open life science journal F1000Research.

% Please note that this template results in a draft pre-submission PDF document.
% Articles will be professionally typeset when accepted for publication.

% We hope you find the F1000Research writeLaTeX template useful,
% please let us know if you have any feedback using the help menu above.


\end{document}